{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ECLARE on sample paired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import environment variables from YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the environment variables set by the script are accessible in this notebook\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Run the export_env_variables.sh script and capture the output\n",
    "result = subprocess.run(['bash', '-c', 'config/export_env_variables.sh config/config.yaml'], capture_output=True, text=True)\n",
    "\n",
    "# Parse the output and set the environment variables in the current Python environment\n",
    "for line in result.stdout.splitlines()[2:]:\n",
    "    key, value = line.split('=', 1)\n",
    "    os.environ[key] = value\n",
    "\n",
    "# Verify that the environment variables are set\n",
    "print(\"ECLARE_ROOT:\", os.environ.get(\"ECLARE_ROOT\"))\n",
    "print(\"OUTPATH:\", os.environ.get(\"OUTPATH\"))\n",
    "print(\"DATAPATH:\", os.environ.get(\"DATAPATH\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to DATAPATH\n",
    "os.chdir(os.environ[\"DATAPATH\"])\n",
    "DATAPATH_TMP = os.environ[\"DATAPATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download sample data from Zenodo (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Download the data from the DOI link\n",
    "!wget https://zenodo.org/records/14799100/files/eclare_sample_zenodo.zip?download=1 -O eclare_data.zip\n",
    "\n",
    "# Unzip the downloaded data\n",
    "!unzip eclare_data.zip -d eclare_data\n",
    "!unzip eclare_data/eclare_sample_zenodo.zip  # takes about 15 minutes @ 5.67 Mb/s\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite the DATAPATH environment variable to the path of the downloaded data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DATAPATH\"] = os.path.join(DATAPATH_TMP, \"eclare_data\", \"eclare_sample_zenodo\")\n",
    "# generally, os.environ[\"DATAPATH\"] = os.path.join(\"/path/to/sample/data\", \"eclare_sample_zenodo\")\n",
    "\n",
    "print(\"DATAPATH: \", os.environ[\"DATAPATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: train CLIP teacher models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got to ECLARE_ROOT\n",
    "os.chdir(os.environ[\"ECLARE_ROOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clip_samples.sh\n",
    "\n",
    "os.environ['N_EPOCHS'] = '5'\n",
    "\n",
    "!${ECLARE_ROOT}/scripts/clip_scripts/clip_samples.sh $N_EPOCHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: perform multi-teacher distillation (ECLARE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to ECLARE_ROOT (in case not already there)\n",
    "os.chdir(os.environ[\"ECLARE_ROOT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the Job ID related to the CLIP teacher models. Should be shown in the first line output by clip_samples.sh, e.g.:<br>\n",
    "\n",
    "Job ID: clip_03173230\n",
    "\n",
    "Can also run code below to identify most common directory in OUTPATH:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most recent directory in OUTPATH that starts with \"clip_\"\n",
    "from glob import glob\n",
    "clip_dirs = glob(os.path.join(os.environ[\"OUTPATH\"], \"clip_*\"))\n",
    "if clip_dirs:\n",
    "    latest_clip_dir = max(clip_dirs, key=os.path.getmtime)\n",
    "    clip_job_id = os.path.basename(latest_clip_dir)\n",
    "    print(f\"Most recent CLIP job directory, assigned to clip_job_id: {clip_job_id}\")\n",
    "else:\n",
    "    print(\"No CLIP job directories found in OUTPATH\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ECLARE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run eclare_samples.sh\n",
    "\n",
    "os.environ['N_EPOCHS'] = '5'\n",
    "os.environ['CLIP_JOB_ID'] = clip_job_id.split('_')[1]  # only keep digits\n",
    "\n",
    "!${ECLARE_ROOT}/scripts/eclare_scripts/eclare_samples.sh $N_EPOCHS $CLIP_JOB_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get most recent ECLARE job ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most recent directory in OUTPATH that starts with \"eclare_\"\n",
    "from glob import glob\n",
    "eclare_dirs = glob(os.path.join(os.environ[\"OUTPATH\"], \"eclare_*\"))\n",
    "if eclare_dirs:\n",
    "    latest_eclare_dir = max(eclare_dirs, key=os.path.getmtime)\n",
    "    eclare_job_id = os.path.basename(latest_eclare_dir)\n",
    "\n",
    "print(f\"Most recent ECLARE job directory, assigned to eclare_job_id: {eclare_job_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for importing data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from eclare import CLIP\n",
    "from eclare import load_CLIP_model\n",
    "\n",
    "\n",
    "def get_metrics(method, job_id, target_only=False):\n",
    "\n",
    "    method_job_id = f'{method}_{job_id}'\n",
    "    paths_root = os.path.join(os.environ['OUTPATH'], method_job_id)\n",
    "\n",
    "    ## retain leaf directories only\n",
    "    paths, all_data_source, all_data_target = [], [], []\n",
    "    paths = glob(os.path.join(paths_root, '**', '**', '**', f'*_metrics_target_valid.csv'))\n",
    "\n",
    "    ## For scMulticlip, will not find metrics with previous command\n",
    "    if paths == []:\n",
    "        paths = glob(os.path.join(paths_root, '**', '**', f'*_metrics_target_valid.csv'))\n",
    "    #for dirpath, dirnames, filenames in os.walk(paths_root): paths.append(dirpath) if not dirnames else None\n",
    "\n",
    "    paths = sorted(paths)\n",
    "\n",
    "    for path in paths:\n",
    "\n",
    "        path = os.path.dirname(path)\n",
    "\n",
    "        ## Get source and target dataset names\n",
    "        path_split = path.split('/')\n",
    "        method_job_id_idx = np.where([split==method_job_id for split in path_split])[0][0]\n",
    "\n",
    "        target = path_split[method_job_id_idx + 1]\n",
    "        source = path_split[method_job_id_idx + 2] if len(path_split) > method_job_id_idx + 2 else None\n",
    "        if source in ['0', '1', '2']: source = None\n",
    "\n",
    "        ## Read metrics\n",
    "        try:\n",
    "            metrics_target_valid = pd.read_csv(glob(os.path.join(path, f'*_metrics_target_valid.csv'))[0], index_col=0)\n",
    "            metrics_source_valid = None if target_only else pd.read_csv(glob(os.path.join(path, f'*_metrics_source_valid.csv'))[0], index_col=0)\n",
    "        except:\n",
    "            print(f'Error reading {path}')\n",
    "            continue\n",
    "\n",
    "        ## Drop foscttm_score_ct & rank_score\n",
    "        if 'foscttm_score_ct' in metrics_target_valid.index:\n",
    "            metrics_target_valid = metrics_target_valid.drop(index=['foscttm_score_ct'])\n",
    "            if ('foscttm_score_ct' in metrics_source_valid.index) and (not target_only):\n",
    "                metrics_source_valid = metrics_source_valid.drop(index=['foscttm_score_ct'])\n",
    "\n",
    "        if 'rank_score' in metrics_target_valid.index:\n",
    "            metrics_target_valid = metrics_target_valid.drop(index=['rank_score'])\n",
    "            if ('rank_score' in metrics_source_valid.index) and (not target_only):\n",
    "                metrics_source_valid = metrics_source_valid.drop(index=['rank_score'])\n",
    "\n",
    "        ## Transpose, such that metrics as columns rather than indices\n",
    "        metrics_target_valid = metrics_target_valid.T\n",
    "        if not target_only:\n",
    "            metrics_source_valid = metrics_source_valid.T\n",
    "\n",
    "        ## Add target dataset names, then append to all_data\n",
    "        metrics_target_valid['target'] = target\n",
    "        metrics_target_valid['source'] = source\n",
    "        all_data_target.append(metrics_target_valid)\n",
    "\n",
    "        if not target_only:\n",
    "            metrics_source_valid['target'] = target\n",
    "            metrics_source_valid['source'] = source\n",
    "            all_data_source.append(metrics_source_valid)\n",
    "\n",
    "    ## Concatenate all_data and set multi-index\n",
    "    target_metrics_valid_df = pd.concat(all_data_target).set_index(['source', 'target'])\n",
    "    target_metrics_valid_df = target_metrics_valid_df.astype(float)\n",
    "\n",
    "    ## TEMPORARY\n",
    "    if 'foscttm' in target_metrics_valid_df.columns:\n",
    "        target_metrics_valid_df = target_metrics_valid_df.rename(columns={'foscttm': 'foscttm_score'})\n",
    "\n",
    "    if 'ilisi' in target_metrics_valid_df.columns:\n",
    "        target_metrics_valid_df = target_metrics_valid_df.rename(columns={'ilisi': 'ilisis'})\n",
    "\n",
    "    if 'clisi' in target_metrics_valid_df.columns:\n",
    "        target_metrics_valid_df = target_metrics_valid_df.rename(columns={'clisi': 'clisis'})\n",
    "\n",
    "    ## Replace foscttm_score by 1 - foscttm_score\n",
    "    if 'foscttm_score' in target_metrics_valid_df.columns:\n",
    "        target_metrics_valid_df['1 - foscttm_score'] = 1 - target_metrics_valid_df['foscttm_score']\n",
    "        target_metrics_valid_df = target_metrics_valid_df.drop(columns=['foscttm_score'])\n",
    "\n",
    "    if target_only:\n",
    "        return target_metrics_valid_df\n",
    "\n",
    "    ## --- SOURCE --- ##\n",
    "    source_metrics_valid_df = pd.concat(all_data_source).set_index(['source', 'target'])\n",
    "    source_metrics_valid_df = source_metrics_valid_df.astype(float)\n",
    "\n",
    "    ## TEMPORARY\n",
    "    if 'foscttm' in source_metrics_valid_df.columns:\n",
    "        source_metrics_valid_df = source_metrics_valid_df.rename(columns={'foscttm': 'foscttm_score'})\n",
    "\n",
    "    if 'ilisi' in source_metrics_valid_df.columns:\n",
    "        source_metrics_valid_df = source_metrics_valid_df.rename(columns={'ilisi': 'ilisis'})\n",
    "\n",
    "    if 'clisi' in source_metrics_valid_df.columns:\n",
    "        source_metrics_valid_df = source_metrics_valid_df.rename(columns={'clisi': 'clisis'})\n",
    "\n",
    "    ## Replace foscttm_score by 1 - foscttm_score\n",
    "    if 'foscttm_score' in source_metrics_valid_df.columns:\n",
    "        source_metrics_valid_df['1 - foscttm_score'] = 1 - source_metrics_valid_df['foscttm_score']\n",
    "        source_metrics_valid_df = source_metrics_valid_df.drop(columns=['foscttm_score'])\n",
    "\n",
    "    ## Create source-only metrics dataframe, and sort according to existing order of target in original df\n",
    "    source_only_metrics_valid_df = source_metrics_valid_df.reset_index(level='target').drop(columns='target')\n",
    "    source_only_metrics_valid_df = source_only_metrics_valid_df.loc[source_metrics_valid_df.index.get_level_values(0).unique().values]\n",
    "\n",
    "    return source_metrics_valid_df, target_metrics_valid_df, source_only_metrics_valid_df\n",
    "\n",
    "def compare_dataframes_target(dataframes, value_column, dataset_labels, target_source_combinations, ax=None):\n",
    "\n",
    "    dataset_label_mapper = {\n",
    "        'AD_Anderson_et_al': 'DLPFC_Anderson',\n",
    "        'PD_Adams_et_al': 'Midbrain_Adams',\n",
    "        'human_dlpfc': 'DLPFC_Ma',\n",
    "        'roussos': 'PFC_Zhu',\n",
    "        'mdd': 'MDD'\n",
    "    }\n",
    "\n",
    "    hue_order = list(dataset_label_mapper.values())\n",
    "\n",
    "    combined_df = pd.concat(\n",
    "        [df.assign(dataset=label).rename(index=dataset_label_mapper)\n",
    "         for df, label in zip(dataframes, dataset_labels)]\n",
    "    ).reset_index()\n",
    "\n",
    "    unique_sources = sorted(combined_df[\"source\"].unique(), key=lambda item: (math.isnan(item), item) if isinstance(item, float) else (False, item))\n",
    "    unique_targets = sorted(combined_df[\"target\"].unique())\n",
    "\n",
    "    colors = sns.color_palette(\"Dark2\", 7)\n",
    "    markers = ['*', 's', '^', 'P', 'D', 'v', '<', '>']\n",
    "\n",
    "    #target_to_color = {target: colors[i] if len(unique_targets)>1 else 'black' for i, target in enumerate(unique_targets)}\n",
    "    if len(unique_targets) > 1:\n",
    "        target_to_color = {hue_order[i]: colors[i] if len(unique_targets)>1 else 'black' for i in range(len(unique_targets))}\n",
    "    elif unique_targets[0] == 'MDD':\n",
    "        target_to_color = {unique_targets[0]: colors[6]}\n",
    "\n",
    "    if target_source_combinations:\n",
    "        source_to_marker = {source: markers[i % len(markers)] for i, source in enumerate(unique_sources)}\n",
    "    else:\n",
    "        source_to_marker = {source: 'o' for i, source in enumerate(unique_sources)}\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    sns.boxplot(\n",
    "        x=\"dataset\",\n",
    "        y=value_column,\n",
    "        data=combined_df,\n",
    "        ax=ax,\n",
    "        color=\"lightgray\",\n",
    "        showfliers=False,\n",
    "        boxprops=dict(alpha=0.4),\n",
    "        whiskerprops=dict(alpha=0.4),\n",
    "        capprops=dict(alpha=0.4),\n",
    "        medianprops=dict(alpha=0.7)\n",
    "    ).tick_params(axis='x', rotation=30)\n",
    "    ax.set_xlabel(\"method\")\n",
    "    ax.yaxis.set_minor_locator(plt.MultipleLocator(0.05))\n",
    "\n",
    "    # Add scatter points with jitter\n",
    "    dataset_positions = combined_df[\"dataset\"].unique()\n",
    "    position_mapping = {dataset: i for i, dataset in enumerate(dataset_positions)}\n",
    "    for target in unique_targets:\n",
    "        for dataset in dataset_positions:\n",
    "            for source in unique_sources:\n",
    "\n",
    "                if pd.isna(source):\n",
    "                    subset = combined_df[(combined_df[\"target\"] == target) &\n",
    "                                        (combined_df[\"source\"].isna()) &\n",
    "                                        (combined_df[\"dataset\"] == dataset)]\n",
    "                    x_position = position_mapping[dataset]\n",
    "                    jitter = np.random.uniform(-0.15, 0.15, size=len(subset))\n",
    "                    jittered_positions = x_position + jitter\n",
    "                    ax.scatter(\n",
    "                        x=jittered_positions,\n",
    "                        y=subset[value_column],\n",
    "                        color=target_to_color[target],\n",
    "                        marker='o',\n",
    "                        edgecolor=None,\n",
    "                        s=50,\n",
    "                        zorder=10,\n",
    "                        alpha=0.4\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    subset = combined_df[(combined_df[\"source\"] == source) &\n",
    "                                        (combined_df[\"target\"] == target) &\n",
    "                                        (combined_df[\"dataset\"] == dataset)]\n",
    "                    x_position = position_mapping[dataset]\n",
    "                    jitter = np.random.uniform(-0.15, 0.15, size=len(subset))\n",
    "                    jittered_positions = x_position + jitter\n",
    "                    ax.scatter(\n",
    "                        x=jittered_positions,\n",
    "                        y=subset[value_column],\n",
    "                        color=target_to_color[target],\n",
    "                        marker=source_to_marker[source],\n",
    "                        edgecolor=None,\n",
    "                        s=50 if source_to_marker[source] == '*' else 50,\n",
    "                        zorder=10,\n",
    "                        alpha=0.4\n",
    "                    )\n",
    "\n",
    "    return source_to_marker, target_to_color\n",
    "    \n",
    "def combined_plot(target_dataframes, value_columns, dataset_labels, target_source_combinations=False, source_only_dataframes=None, figsize=(6,6)):\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    if source_only_dataframes is None:\n",
    "        fig, axs = plt.subplots(1, len(value_columns), figsize=figsize, sharex=True, squeeze=False)\n",
    "    else:\n",
    "        fig, axs = plt.subplots(2, len(value_columns), figsize=figsize, sharey='col')\n",
    "\n",
    "    for value_column, ax in zip(value_columns, axs[-1]):\n",
    "        # Pass axes to the individual plotting functions\n",
    "        source_to_marker, target_to_color = compare_dataframes_target(target_dataframes, value_column, dataset_labels, target_source_combinations, ax=ax)\n",
    "\n",
    "    # Create handles for color-coding by target\n",
    "    color_handles = [\n",
    "        plt.Line2D([0], [0], marker='D' if target_source_combinations else 'o', color=color, label=f\"{target}\", linestyle='None', markersize=10)\n",
    "        for target, color in target_to_color.items()\n",
    "    ]\n",
    "\n",
    "    # Create handles for marker-coding by source\n",
    "    marker_handles = [\n",
    "        plt.Line2D([0], [0], marker=marker, color='black', label=f\"{source}\", linestyle='None', markersize=10)\n",
    "        if not pd.isna(source) else\n",
    "        plt.Line2D([0], [0], marker='o', color='black', label=f\"all sources (ensemble)\", linestyle='None', markersize=10)\n",
    "        for source, marker in source_to_marker.items()\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Add labels for color and marker sections\n",
    "    if ( (len(set(source_to_marker.values())) > 1) and ( len(set(target_to_color.keys())) > 1) ):\n",
    "\n",
    "        # Add a blank entry and title for marker section\n",
    "        combined_handles = (\n",
    "            [plt.Line2D([0], [0], color='none', label=\"Target, by color:\")] +\n",
    "            color_handles +\n",
    "            [plt.Line2D([0], [0], color='none', label=\"\")] +  # Blank separator\n",
    "            [plt.Line2D([0], [0], color='none', label=\"Source, by marker:\")] +  # Title for the second block\n",
    "            marker_handles\n",
    "        )\n",
    "        combined_labels = (\n",
    "            ['Target, by color:'] +\n",
    "            [f\"{target}\" for target in target_to_color.keys()] +\n",
    "            [''] +  # Blank separator\n",
    "            [\"Source, by marker:\"] +\n",
    "            [   f\"{source}\" if not pd.isna(source) else \"all sources (ensemble)\"\n",
    "                for source in source_to_marker.keys()]\n",
    "        )\n",
    "\n",
    "    elif ( (len(set(source_to_marker.values())) == 1) and ( len(set(target_to_color.keys())) > 1) ):\n",
    "        combined_handles = (\n",
    "            [plt.Line2D([0], [0], color='none', label=\"Target, by color:\")] +\n",
    "            color_handles\n",
    "        )\n",
    "        combined_labels = (\n",
    "            ['Target, by color:'] +\n",
    "            [f\"{target}\" for target in target_to_color.keys()]\n",
    "        )\n",
    "    elif ( (len(set(source_to_marker.values())) > 1) and ( len(set(target_to_color.keys())) == 1) ):\n",
    "        combined_handles = (\n",
    "            [plt.Line2D([0], [0], color='none', label=\"Source, by marker:\")] +\n",
    "            marker_handles\n",
    "        )\n",
    "        combined_labels = (\n",
    "            [\"Source, by marker:\"] +\n",
    "            [   f\"{source}\" if not pd.isna(source) else \"all sources (ensemble)\"\n",
    "                for source in source_to_marker.keys()]\n",
    "        )\n",
    "    else:\n",
    "        combined_handles = None\n",
    "        combined_labels = None\n",
    "\n",
    "    axs[-1,-1].legend(\n",
    "        handles=combined_handles,\n",
    "        labels=combined_labels,\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc='upper left',\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    # Set a title for the entire figure\n",
    "    for value_column, ax, letter in zip(value_columns, axs[0], ascii_uppercase):\n",
    "\n",
    "        ax.set_title(f\"{letter}) {value_column}\")\n",
    "        ax.set_xlabel(None)\n",
    "        ax.set_ylabel(None)\n",
    "\n",
    "        if source_only_dataframes is not None:\n",
    "            source_to_target = compare_dataframes_source_only(source_only_dataframes, value_column, dataset_labels, ax=ax)\n",
    "\n",
    "            axs[0,0].set_ylabel(f\"source to source\")\n",
    "            axs[-1,0].set_ylabel(f\"source to target\")\n",
    "\n",
    "            combined_handles = (\n",
    "                [plt.Line2D([0], [0], color='none', label=\"Source, by color:\")] +\n",
    "                color_handles\n",
    "            )\n",
    "            combined_labels = (\n",
    "                ['Source, by color:'] +\n",
    "                [f\"{target}\" for target in target_to_color.keys()]\n",
    "            )\n",
    "\n",
    "            axs[0,-1].legend(\n",
    "                handles=combined_handles,\n",
    "                labels=combined_labels,\n",
    "                bbox_to_anchor=(1.05, 1),\n",
    "                loc='upper left',\n",
    "                frameon=False\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "def load_clip_and_eclare_model(student_model_path, best_multiclip_idx, device='cpu', genes_by_peaks_str='6816_by_55284'):\n",
    "    student_model_args_dict = torch.load(student_model_path, map_location=device)\n",
    "\n",
    "    clip_job_id = student_model_args_dict['args'].clip_job_id\n",
    "    model_paths = glob(os.path.join(os.environ['OUTPATH'], f'clip_{clip_job_id}/PFC_Zhu/**/{best_multiclip_idx}/model.pt'))\n",
    "\n",
    "    teacher_models = {}\n",
    "    for model_path in model_paths:  \n",
    "        teacher_model, teacher_clip_model_args_dict = load_CLIP_model(model_path, device=device)\n",
    "        teacher_models[teacher_model.args.source_dataset] = teacher_model.eval()\n",
    "\n",
    "    # student copies from last teacher model, makes no difference\n",
    "    student_clip_model_args_dict = copy.deepcopy(teacher_clip_model_args_dict)\n",
    "\n",
    "    student_clip_model_args_dict['args'].source_dataset = 'PFC_Zhu'\n",
    "    student_clip_model_args_dict['args'].target_dataset = 'mdd'\n",
    "\n",
    "    student_clip_model_args_dict['args'].genes_by_peaks_str = genes_by_peaks_str\n",
    "    student_clip_model_args_dict['n_genes'] = int(genes_by_peaks_str.split('_')[0])\n",
    "    student_clip_model_args_dict['n_peaks'] = int(genes_by_peaks_str.split('_')[-1])\n",
    "    student_clip_model_args_dict['tuned_hyperparameters']['params_num_layers'] = 2\n",
    "    student_clip_model_args_dict['pretrain'] = student_clip_model_args_dict['rna_valid_idx']  = student_clip_model_args_dict['atac_valid_idx'] = None\n",
    "\n",
    "    student_model = CLIP(**student_clip_model_args_dict, trial=None)\n",
    "    \n",
    "    student_model.load_state_dict(student_model_args_dict['model_state_dict'])\n",
    "    student_model.eval()\n",
    "\n",
    "    return teacher_models, student_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics\n",
    "clip_job_id_split = clip_job_id.split('_')[1]\n",
    "eclare_job_id_split = eclare_job_id.split('_')[1]\n",
    "\n",
    "source_df_clip, target_df_clip, source_only_df_clip = get_metrics('clip', clip_job_id_split)   # may need to rename 'triplet_align_<job_id>' by 'clip_<job_id>'\n",
    "target_df_multiclip = get_metrics('eclare', eclare_job_id_split, target_only=True) # may need to rename 'multisource_align_<job_id>' by 'multiclip_<job_id>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher CLIP and student ECLARE models\n",
    "best_multiclip_idx= str(target_df_multiclip['ilisis'].droplevel(0).argmax())\n",
    "paths_root = os.path.join(os.environ['OUTPATH'], eclare_job_id)\n",
    "student_model_path = os.path.join(paths_root, 'PFC_Zhu', best_multiclip_idx, 'student_model.pt')\n",
    "\n",
    "teacher_models, student_model = load_clip_and_eclare_model(student_model_path, best_multiclip_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nuclei and latents\n",
    "from eclare.setup_utils import pfc_zhu_setup\n",
    "from eclare.post_hoc_utils import get_latents\n",
    "\n",
    "# Teacher data\n",
    "teacher_rnas, teacher_atacs, teacher_rna_latents_dict, teacher_atac_latents_dict = {}, {}, {}, {}\n",
    "\n",
    "for source_dataset, teacher_model in teacher_models.items():\n",
    "    \n",
    "    teacher_rna, teacher_atac, cell_group, _, _, _, _ = pfc_zhu_setup(teacher_model.args, pretrain=None, return_type='data')\n",
    "    teacher_rnas[source_dataset] = teacher_rna\n",
    "    teacher_atacs[source_dataset] = teacher_atac\n",
    "\n",
    "    teacher_rna_latents, teacher_atac_latents = get_latents(teacher_model, teacher_rna, teacher_atac, return_tensor=True)\n",
    "    teacher_rna_latents_dict[source_dataset] = teacher_rna_latents\n",
    "    teacher_atac_latents_dict[source_dataset] = teacher_atac_latents\n",
    "\n",
    "# Student data\n",
    "student_rna, student_atac, cell_group, _, _, _, _ = pfc_zhu_setup(student_model.args, pretrain=None, return_type='data')\n",
    "student_rna_latents, student_atac_latents = get_latents(student_model, student_rna, student_atac, return_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot UMAP embeddings for teachers and student\n",
    "from eclare.post_hoc_utils import plot_umap_embeddings\n",
    "from eclare.post_hoc_utils import create_celltype_palette\n",
    "\n",
    "color_map_ct = create_celltype_palette(teacher_rna.obs[cell_group].values, teacher_atac.obs[cell_group].values, plot_color_palette=False)\n",
    "\n",
    "# teachers\n",
    "for source_dataset in teacher_rnas.keys():\n",
    "    plot_umap_embeddings(teacher_rna_latents_dict[source_dataset], teacher_atac_latents_dict[source_dataset], teacher_rnas[source_dataset].obs[cell_group].values, teacher_atacs[source_dataset].obs[cell_group].values, None, None, color_map_ct)\n",
    "    plt.suptitle(f\"PFC_Zhu embeddings using teacher model (source: {source_dataset})\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# student\n",
    "plot_umap_embeddings(student_rna_latents, student_atac_latents, student_rna.obs[cell_group].values, student_atac.obs[cell_group].values, None, None, color_map_ct)\n",
    "plt.suptitle(f\"PFC_Zhu embeddings using student model\"); plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eclare_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
